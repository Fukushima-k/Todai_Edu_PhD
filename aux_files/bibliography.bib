@article{arai2020japanese,
  title   = {思考力を測ろうとする多枝選択式問題の解答過程に関する調査に基づく実証分析},
  author  = {荒井 清佳},
  journal = {日本テスト学会誌},
  volume  = {16},
  number  = {1},
  pages   = {13-30},
  year    = {2020},
  doi     = {10.24690/jart.16.1_13}
}
@article{arai2020think,
  title     = {An empirical analysis based on surveys on the answering process of
               multiple-choice items to measure the ability to think},
  author    = {Arai, Sayaka},
  journal   = {Japanese journal for research on testing},
  volume    = {16},
  number    = {1},
  pages     = {14--30},
  year      = {2020},
  publisher = {日本テスト学会}
}
@article{asquith2022investigation,
  title   = {An Investigation into the Roles of Guessing and Partial Knowledge in the Vocabulary Size Test.},
  author  = {Asquith, Steven},
  journal = {Teaching English as a Second Language Electronic Journal},
  volume  = {26},
  number  = {3},
  year    = {2022}
}
@article{barr2013using,
  title     = {Using confidence-based marking in a laboratory setting: A tool for student self-assessment and learning},
  author    = {Barr, Deborah A and Burke, Jeanmarie R},
  journal   = {Journal of Chiropractic Education},
  volume    = {27},
  number    = {1},
  pages     = {21--26},
  year      = {2013},
  publisher = {the Association of Chiropractic Colleges},
  doi       = {10.7899/JCE-12-018}
}
@article{ben1997comparative,
  title     = {A comparative study of measures of partial knowledge in multiple-choice tests},
  author    = {Ben-Simon, Anat and Budescu, David V and Nevo, Baruch},
  journal   = {Applied Psychological Measurement},
  volume    = {21},
  number    = {1},
  pages     = {65--88},
  year      = {1997},
  publisher = {SAGE PUBLICATIONS, INC. 2455 Teller Road, Thousand Oaks, CA 91320}
}
@article{bennett2011formative,
  title     = {Formative assessment: A critical review},
  author    = {Bennett, Randy Elliot},
  journal   = {Assessment in education: principles, policy \& practice},
  volume    = {18},
  number    = {1},
  pages     = {5--25},
  year = {2011},
  publisher = {Routledge},
  doi = {10.1080/0969594X.2010.513678},
}
@manual{BILOGMGGuide,
  author = {{Scientific Software International, Inc.}},
  year   = {2003},
  title  = {{BILOG-MG G}uide},
  url    = {https://ssicentral.com/index.php/products/bilogmg-gen/},
  note   = {Accessed: 2024/11/18}
}
@article{Black1998-yt,
  title     = {Assessment and classroom learning},
  author    = {Black, Paul and Wiliam, Dylan},
  journal   = {Assessment in Education},
  publisher = {Informa UK Limited},
  volume    = 5,
  number    = 1,
  pages     = {7--74},
  abstract  = {ABSTRACT This article is a review of the literature on classroom
               formative assessment. Several studies show firm evidence that
               innovations designed to strengthen the frequent feedback that
               students receive about their learning yield substantial learning
               gains. The perceptions of students and their role in
               self‐assessment are considered alongside analysis of the
               strategies used by teachers and the formative strategies
               incorporated in such systemic approaches as mastery learning.
               There follows a more detailed and theoretical analysis of the
               nature of feedback, which provides a basis for a discussion of
               the development of theoretical models for formative assessment
               and of the prospects for the improvement of practice.},
  month     = mar,
  year      = 1998,
  language  = {en},
  doi = {10.1080/0969595980050102},
}
@article{bock1972NRM,
  title     = {Estimating item parameters and latent ability when responses are scored in two or more nominal categories},
  author    = {Bock, Darrell R},
  journal   = {Psychometrika},
  volume    = {37},
  number    = {1},
  pages     = {29--51},
  year      = {1972},
  publisher = {Springer},
  doi       = {10.1007/BF02291411},
}
@article{bradshaw2014combining,
  title     = {Combining item response theory and diagnostic classification models: A psychometric model for scaling ability and diagnosing misconceptions},
  author    = {Bradshaw, Laine and Templin, Jonathan},
  journal   = {Psychometrika},
  volume    = {79},
  pages     = {403--425},
  year      = {2014},
  publisher = {Springer},
  doi       = {10.1007/s11336-013-9350-4}
}
@article{brown2022modification,
  title     = {Modification indices for diagnostic classification models},
  author    = {Brown, Christy and Templin, Jonathan},
  journal   = {Multivariate behavioral research},
  pages     = {1--18},
  year      = {2022},
  publisher = {Taylor \& Francis},
  doi       = {10.1080/00273171.2022.2049672}
}


% general CDM
@article{bush2001multiple,
  title     = {A multiple choice test that rewards partial knowledge},
  author    = {Bush, Martin},
  journal   = {Journal of Further and Higher education},
  volume    = {25},
  number    = {2},
  pages     = {157--163},
  year      = {2001},
  publisher = {Taylor \& Francis},
  doi       = {10.1080/03098770120050828}
}
@article{butler2018multiple,
  title     = {Multiple-choice testing in education: Are the best practices for assessment also good for learning?},
  author    = {Butler, Andrew C},
  journal   = {Journal of Applied Research in Memory and Cognition},
  volume    = {7},
  number    = {3},
  pages     = {323--331},
  year      = {2018},
  publisher = {Elsevier},
  doi       = {10.1016/j.jarmac.2018.07.002}
}
@article{chen2017GNDM,
  title     = {Test designs and modeling under the general nominal diagnosis model framework},
  author    = {Chen, Jinsong and Zhou, Hui},
  journal   = {PloS one},
  volume    = {12},
  number    = {6},
  pages     = {e0180016},
  year      = {2017},
  publisher = {Public Library of Science San Francisco, CA USA},
  doi       = {10.1371/journal.pone.0180016}
}

@article{Chiu2013-yn,
  title     = {A nonparametric approach to cognitive diagnosis by proximity to
               ideal response patterns},
  author    = {Chiu, Chia-Yi and Douglas, Jeff},
  journal   = {Journal of Classification},
  publisher = {Springer Science and Business Media LLC},
  volume    = 30,
  number    = 2,
  pages     = {225--250},
  abstract  = {A trend in educational testing is to go beyond unidimensional
               scoring and provide a more complete profile of skills that have
               been mastered and those that have not. To achieve this, cognitive
               diagnosis models have been developed that can be viewed as
               restricted latent class models. Diagnosis of class membership is
               the statistical objective of these models. As an alternative to
               latent class modeling, a nonparametric procedure is introduced
               that only requires specification of an item-by-attribute
               association matrix, and classifies according to minimizing a
               distance measure between observed responses, and the ideal
               response for a given attribute profile that would be implied by
               the item-by-attribute association matrix. This procedure requires
               no statistical parameter estimation, and can be used on a sample
               size as small as 1. Heuristic arguments are given for why the
               nonparametric procedure should be effective under various
               possible cognitive diagnosis models for data generation.
               Simulation studies compare classification rates with parametric
               models, and consider a variety of distance measures, data
               generation models, and the effects of model misspecification. A
               real data example is provided with an analysis of agreement
               between the nonparametric method and parametric approaches.},
  month     = jul,
  year      = 2013,
  language  = {en},
  doi = {10.1007/s00357-013-9132-9},
}

% text
@article{Chiu2018-ko,
  title     = {Cognitive diagnosis for small educational programs: The general
               nonparametric classification method},
  author    = {Chiu, Chia-Yi and Sun, Yan and Bian, Yanhong},
  journal   = {Psychometrika},
  publisher = {Springer Nature B.V.},
  volume    = 83,
  number    = 2,
  pages     = {355--375},
  abstract  = {The focus of cognitive diagnosis (CD) is on evaluating an
               examinee's strengths and weaknesses in terms of cognitive skills
               learned and skills that need study. Current methods for fitting
               CD models (CDMs) work well for large-scale assessments, where the
               data of hundreds or thousands of examinees are available.
               However, the development of CD-based assessment tools that can be
               used in small-scale test settings, say, for monitoring the
               instruction and learning process at the classroom level has not
               kept up with the rapid pace at which research and development
               proceeded for large-scale assessments. The main reason is that
               the sample sizes of the small-scale test settings are simply too
               small to guarantee the reliable estimation of item parameters and
               examinees' proficiency class membership. In this article, a
               general nonparametric classification (GNPC) method that allows
               for assigning examinees to the correct proficiency classes with a
               high rate when sample sizes are at the classroom level is
               proposed as an extension of the nonparametric classification
               (NPC) method (Chiu and Douglas in J Classif 30:225-250, 2013).
               The proposed method remedies the shortcomings of the NPC method
               and can accommodate any CDM. The theoretical justification and
               the empirical studies are presented based on the saturated
               general CDMs, supporting the legitimacy of using the GNPC method
               with any CDM. The results from the simulation studies and real
               data analysis show that the GNPC method outperforms the general
               CDMs when samples are small.},
  month     = jun,
  year      = 2018,
  keywords  = {G-DINA; LCDM; cognitive diagnosis; cognitive diagnostic models;
               general nonparametric classification method},
  language  = {en},
  doi = {10.1007/s11336-017-9595-4}
}

@article{chiu2019consistnonpara,
  title     = {Consistency theory for the general nonparametric classification method},
  author    = {Chiu, Chia-Yi and K{\"o}hn, Hans-Friedrich},
  journal   = {Psychometrika},
  volume    = {84},
  pages     = {830--845},
  year      = {2019},
  publisher = {Springer},
  doi = {10.1007/s11336-019-09667-7}
}
@article{fukushima2024modeling,
  title={Modeling Partial Knowledge in Multiple-Choice Cognitive Diagnostic Assessment},
  author={Fukushima, Kentaro and Uchida, Nao and Okada, Kensuke},
  journal={Journal of Educational and Behavioral Statistics},
  year={2024},
  note={Advance online publication},
  doi={10.3102/10769986241245707}
}

@book{Christensen2006LLM,
  title     = {{Log-Linear} Models and Logistic Regression},
  author    = {Christensen, Ronald},
  abstract  = {As the new title indicates, this second edition of Log-Linear
               Models has been modi?ed to place greater emphasis on logistic
               regression. In addition to new material, the book has been
               radically rearranged. The fundamental material is contained in
               Chapters 1-4. Intermediate topics are presented in Chapters 5
               through 8. Generalized linear models are presented in Ch- ter 9.
               The matrix approach to log-linear models and logistic regression
               is presented in Chapters 10-12, with Chapters 10 and 11 at the
               applied Ph.D. level and Chapter 12 doing theory at the Ph.D.
               level. The largest single addition to the book is Chapter 13 on
               Bayesian bi- mial regression. This chapter includes not only
               logistic regression but also probit and complementary log-log
               regression. With the simplicity of the Bayesian approach and the
               ability to do (almost) exact small sample s- tistical inference,
               I personally ?nd it hard to justify doing traditional large
               sample inferences. (Another possibility is to do exact
               conditional inference, but that is another story.)
               Naturally,Ihavecleaneduptheminor?awsinthetextthatIhavefound. All
               examples, theorems, proofs, lemmas, etc. are numbered
               consecutively within each section with no distinctions between
               them, thus Example 2.3.1
               willcomebeforeProposition2.3.2.Exercisesthatdonotappearinasection
               at the end have a separate numbering scheme. Within the section
               in which it appears, an equation is numbered with a single
               value, e.g., equation (1).},
  publisher = {Springer Science \& Business Media},
  month     = apr,
  year      = 2006,
  language  = {en}
}
@article{chung2019gibbs,
  title     = {A {G}ibbs sampling algorithm that estimates the {Q}-matrix for the {DINA} model},
  author    = {Chung, Mengta},
  journal   = {Journal of Mathematical Psychology},
  volume    = {93},
  pages     = {102275},
  year      = {2019},
  publisher = {Elsevier},
  doi       = {10.1016/j.jmp.2019.07.002}
}
@article{coombs1956assessment,
  title     = {The assessment of partial knowledge},
  author    = {Coombs, Clyde H and Milholland, John Edgar and Womer, Frank Burton},
  journal   = {Educational and Psychological Measurement},
  volume    = {16},
  number    = {1},
  pages     = {13--37},
  year      = {1956},
  publisher = {Sage Publications Sage CA: Los Angeles, CA},
  doi       = {10.1177/001316445601600102}
}

@article{culpepper2015bayesian,
  title     = {{Bayesian} estimation of the {DINA} model with {G}ibbs sampling},
  author    = {Culpepper, Steven Andrew},
  journal   = {Journal of Educational and Behavioral Statistics},
  volume    = {40},
  number    = {5},
  pages     = {454--476},
  year      = {2015},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
  doi       = {10.3102/1076998615595403}
}
@article{de2008discrimination_index,
  issn      = {00220655, 17453984},
  url       = {http://www.jstor.org/stable/20461904},
  abstract  = {Most model fit analyses in cognitive diagnosis assume that a {Q} matrix is correct after it has been constructed, without verifying its appropriateness. Consequently, any model misfit attributable to the {Q} matrix cannot be addressed and remedied. To address this concern, this paper proposes an empirically based method of validating a {Q} matrix used in conjunction with the {DINA} model. The proposed method can be implemented with other considerations such as substantive information about the items, or expert knowledge about the domain, to produce a more integrative framework of {Q}-matrix validation. The paper presents the theoretical foundation for the proposed method, develops an algorithm for its practical implementation, and provides real and simulated data applications to examine its viability. Relevant issues regarding the implementation of the method are discussed.},
  author    = {Jimmy de la Torre},
  journal   = {Journal of Educational Measurement},
  number    = {4},
  pages     = {343--362},
  publisher = {[National Council on Measurement in Education, Wiley]},
  title     = {An Empirically Based Method of {Q}-Matrix Validation for the {DINA} Model: Development and Applications},
  volume    = {45},
  year      = {2008}
}
@article{de2009cognitive,
  title     = {A cognitive diagnosis model for cognitively based multiple-choice options},
  author    = {de la Torre, Jimmy},
  journal   = {Applied Psychological Measurement},
  volume    = {33},
  number    = {3},
  pages     = {163--183},
  year      = {2009},
  publisher = {Sage Publications Sage CA: Los Angeles, CA},
  doi       = {10.1177/0146621608320523}
}
@article{de2011generalized,
  title     = {The generalized {DINA} model framework},
  author    = {de la Torre, Jimmy},
  journal   = {Psychometrika},
  volume    = {76},
  number    = {2},
  pages     = {179--199},
  year      = {2011},
  publisher = {Springer},
  doi       = {10.1007/s11336-011-9207-7}
}
@misc{de2022eMC-IMPS,
  title  = {{CDM}s that optimize the diagnostic value of multiple-choice data: Real data applications},
  author = {de la Torre, Jimmy and Qui, X.-L},
  year   = {2022},
  month  = {7},
  note   = {Paper presented at the International Meeting of the Psychometric Society, Bologna, Italy.}
}
@misc{de2022eMC-NCME,
  title  = {A new {CDM} Framework for diagnosing skills and misconceptions for multiple-choice data},
  author = {de la Torre, Jimmy and Qui,  X.-L},
  year   = {2022},
  month  = {4},
  note   = {Paper presented at the annual meeting of the National Council on Measurement in Education, San Diego, CA.}
}



% partial knowledge
@article{dibello2015family,
  title     = {A family of generalized diagnostic classification models for multiple choice option-based scoring},
  author    = {DiBello, Louis V and Henson, Robert A and Stout, William F},
  journal   = {Applied Psychological Measurement},
  volume    = {39},
  number    = {1},
  pages     = {62--79},
  year      = {2015},
  publisher = {Sage Publications Sage CA: Los Angeles, CA},
  doi       = {10.1177/0146621614561315}
}
@phdthesis{Elbulok2021-de,
  title    = {A Cognitively Diagnostic Modeling Approach to Diagnosing
              Misconceptions and Subskills},
  author   = {Elbulok, Musa},
  editor   = {Corter, James},
  abstract = {The objective of the present project was to propose a new
              methodology for measuring misconceptions and subskills
              simultaneously using diagnostic information available from
              incorrect alternatives in multiple-choice tests designed for that
              purpose. Misconceptions are systematic and persistent errors that
              represent a learned intentional incorrect response (Brown \&
              VanLehn, 1980; Ozkan \& Ozkan, 2012). In prior research, Lee and
              Corter (2011) found that classification accuracy for their
              Bayesian Network misconception diagnosis models improved when
              latent higher-order subskills and specific wrong answers were
              included. Here, these contributions are adapted to a cognitively
              diagnostic measurement approach using the multiple-choice
              Deterministic Inputs Noisy ``And'' Gate (MC-DINA) model, first
              developed by de la Torre (2009b), by specifying dependencies
              between attributes to measure latent misconceptions and subskills
              simultaneously. A simulation study was conducted employing the
              proposed methodology (referred to as MC-DINA-H) across sample
              sizes (500, 1000, 2,000, and 5,000 examinees) and test lengths
              (15, 30, and 60 items) conditions. Eight attributes (4
              misconceptions and 4 subskills) were included in the main
              simulation study. Attribute classification accuracy of the
              MC-DINA-H was compared to four less complex models and was found
              to more accurately classify attributes only when the attributes
              were relatively frequently required by multiple-choice options in
              the diagnostic assessment. The findings suggest that each
              attribute should be required by at least 15-20 percent of options
              in the diagnostic assessment.},
  year     = 2021,
  address  = {Ann Arbor, United States},
  school   = {Columbia University}
}
@book{elbulok2021cognitively,
  title     = {A Cognitively Diagnostic Modeling Approach to Diagnosing Misconceptions and Subskills},
  author    = {Elbulok, Musa},
  year      = {2021},
  publisher = {Columbia University},
  address   = {New York}
}
@article{embretson1984,
  title     = {A general latent trait model for response processes},
  author    = {Embretson, Susan},
  journal   = {Psychometrika},
  volume    = {49},
  number    = {2},
  pages     = {175--186},
  year      = {1984},
  publisher = {Springer},
  doi       = {10.1007/BF02294171}
}
@article{frary1980effect,
  title     = {The effect of misinformation, partial information, and guessing on expected multiple-choice test item scores},
  author    = {Frary, Robert B},
  journal   = {Applied Psychological Measurement},
  volume    = {4},
  number    = {1},
  pages     = {79--90},
  year      = {1980},
  publisher = {Sage Publications Sage CA: Thousand Oaks, CA},
  doi       = {10.1177/014662168000400109}
}
@article{frary1982willingness,
  title     = {Willingness to answer multiple-choice questions as manifested both in genuine and in nonsense items},
  author    = {Frary, Robert B and Hutchinson, TP},
  journal   = {Educational and Psychological Measurement},
  volume    = {42},
  number    = {3},
  pages     = {815--821},
  year      = {1982},
  publisher = {Sage Publications Sage CA: Thousand Oaks, CA},
  doi       = {10.1177/001316448204200314}
}

@article{frary1989partial,
  title     = {Partial-credit scoring methods for multiple-choice tests},
  author    = {Frary, Robert B},
  journal   = {Applied Measurement in Education},
  publisher = {Informa UK Limited},
  volume    = 2,
  number    = 1,
  pages     = {79--96},
  month     = jan,
  year      = 1989,
  language  = {en},
  doi = {10.1207/s15324818ame0201_5}
}
@conference{fukushima2021IMPS,
  title     = {Modeling partial knowledge in multiple-choice cognitive diagnostic models.},
  author    = {Fukushima, K and Uchida, N and Okada,},
  booktitle = {International Meeting of Psychometric Society},
  doi       = {https://psyarxiv.com/wefb7/},
  note      = {Preprint is available from \url{https://psyarxiv.com/wefb7/}},
  year      = {2021}
}
@article{fukushima2021japanese,
  title     = {{Q} 行列を付与した多枝選択形式テストの開発―認知診断モデルのための英語の空所補充問題の作成―},
  author    = {福島 健太郎 and 内田 奈緒 and 岡田 謙介},
  journal   = {日本テスト学会誌},
  volume    = {17},
  number    = {1},
  pages     = {45--59},
  year      = {2021},
  publisher = {日本テスト学会},
  doi       = {10.24690/jart.17.1_45}
}
@article{fukushima2021q,
  title     = {Q gyoretsu wo fuyo shita tashisentaku keishiki tesuto no kaihatu --{N}inchi shindan moderu no tame no eigo no kuusho hojuu mondai no sakusei-- [{D}evelopment of a multiple-choice fill-in-the-blanks assessment of {E}nglish with a {Q}-matrix for cognitive diagnostic models]},
  author    = {Fukushima, Kentaro and Uchida, Nao and Okada, Kensuke},
  journal   = {Nihon tesuto gakkaishi},
  volume    = {17},
  number    = {1},
  pages     = {45--59},
  year      = {2021},
  publisher = {日本テスト学会},
  doi       = {10.24690/jart.17.1_45}
}
@article{Gao2021-GPoly,
  title     = {A Class of Cognitive Diagnosis Models for Polytomous Data},
  author    = {Gao, Xuliang and Ma, Wenchao and Wang, Daxun and Cai, Yan and
               Tu, Dongbo},
  abstract  = {This article proposes a class of cognitive diagnosis models
               (CDMs) for polytomously scored items with different link
               functions. Many existing polytomous CDMs can be considered as
               special cases of the proposed class of polytomous CDMs.
               Simulation studies were carried out to investigate the
               feasibility of the proposed CDMs and the performance of several
               information criteria (Akaike?s information criterion [AIC],
               consistent Akaike?s information criterion [CAIC], and Bayesian
               information criterion [BIC]) in model selection. The results
               showed that the parameters of the proposed CDMs could be
               recovered adequately under varied conditions. In addition, CAIC
               and BIC had better performance in selecting the most appropriate
               model than AIC. Finally, a set of real data was analyzed to
               illustrate the application of the proposed CDMs.},
  journal   = {Journal of Educational and Behavioral Statistics},
  publisher = {American Educational Research Association},
  volume    = 46,
  number    = 3,
  pages     = {297--322},
  month     = jun,
  year      = 2021,
  doi = {10.3102/1076998620951986}
}
%Chiu, C.-Y., Douglas, J. A., & Li, X. (2009). Cluster analysis for cognitive diagnosis: Theory and applications. Psychometrika, 74, 633–665.


% PROGRAM  
@article{gardner1995confidence,
  title     = {Confidence assessment in the teaching of basic science},
  author    = {Gardner-Medwin, AR},
  journal   = {ALT-J},
  volume    = {3},
  number    = {1},
  pages     = {80--85},
  year      = {1995},
  publisher = {Taylor \& Francis}
}
@article{gelman1992inference,
  title     = {Inference from iterative simulation using multiple sequences},
  author    = {Gelman, Andrew and Rubin, Donald B},
  journal   = {Statistical science},
  volume    = {7},
  number    = {4},
  pages     = {457--472},
  year      = {1992},
  publisher = {Institute of Mathematical Statistics},
  doi       = {10.1214/ss%2F1177011136},
  url       = {https://www.jstor.org/stable/2246093}
}
@article{Gu2020-jt,
  title     = {Partial identifiability of restricted latent class models},
  author    = {Gu, Yuqi and Xu, Gongjun},
  journal   = {The Annals of Statistics},
  publisher = {Institute of Mathematical Statistics},
  volume    = 48,
  number    = 4,
  pages     = {2082--2107},
  abstract  = {Latent class models have wide applications in social and
               biological sciences. In many applications, prespecified
               restrictions are imposed on the parameter space of latent class
               models, through a design matrix, to reflect practitioners’
               assumptions about how the observed responses depend on subjects’
               latent traits. Though widely used in various fields, such
               restricted latent class models suffer from nonidentifiability due
               to their discreteness nature and complex structure of
               restrictions. This work addresses the fundamental identifiability
               issue of restricted latent class models by developing a general
               framework for strict and partial identifiability of the model
               parameters. Under correct model specification, the developed
               identifiability conditions only depend on the design matrix and
               are easily checkable, which provide useful practical guidelines
               for designing statistically valid diagnostic tests. Furthermore,
               the new theoretical framework is applied to establish, for the
               first time, identifiability of several designs from cognitive
               diagnosis applications.},
  month     = aug,
  year      = 2020,
  keywords  = {$Q$-matrix; 62E10; 62P15; cognitive diagnosis; Identifiability;
               restricted latent class models},
  language  = {en},
  doi = {10.1214/19-AOS1878}
}
% R Core Team. (2016). R: A language and environment for statistical computing. Vienna, Austria: The R Foundation for Statistical Computing.




% CDM basis 
@article{guo2023cognitive,
  title     = {Cognitive Diagnosis Testlet Model for Multiple-Choice Items},
  author    = {Guo, Lei and Zhou, Wenjie and Li, Xiao},
  journal   = {Journal of Educational and Behavioral Statistics},
  pages     = {10769986231165622},
  year      = {2023},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA}
}

% misspecification 

@article{haertel1989using,
  title     = {Using restricted latent class models to map the skill structure of achievement items},
  author    = {Haertel, Edward H},
  journal   = {Journal of Educational Measurement},
  volume    = {26},
  number    = {4},
  pages     = {301--321},
  year      = {1989},
  publisher = {Wiley Online Library},
  doi       = {10.1111/j.1745-3984.1989.tb00336.x}
}




% テスト学会分
@book{hagenaars1993loglinear,
  title     = {Loglinear models with latent variables},
  author    = {Hagenaars, Jacques A},
  number    = {94},
  year      = {1993},
  publisher = {Sage}
}
@book{haladyna2013develop,
  title     = {Developing and Validating Test Items},
  author    = {Haladyna, Thomas M and Rodriguez, Michael C},
  year      = {2013},
  edition   = {1},
  publisher = {Routledge},
  doi       = {https://doi.org/10.4324/9780203850381},
  address   = {New York}
}
@book{hartz2002RRUM,
  title     = {A {Bayesian} framework for the unified model for assessing cognitive abilities: Blending theory with practicality},
  author    = {Hartz, Sarah McConnell},
  year      = {2002},
  publisher = {University of Illinois at Urbana-Champaign}
}
@article{hassmen1994human,
  title     = {Human self-assessment in multiple-choice testing},
  author    = {Hassm{\'e}n, Peter and Hunt, Darwin P},
  journal   = {Journal of Educational Measurement},
  volume    = {31},
  number    = {2},
  pages     = {149--160},
  year      = {1994},
  publisher = {Wiley Online Library},
  doi       = {10.1111/j.1745-3984.1994.tb00440.x}
}
@article{henson2009LCDM,
  title     = {Defining a family of cognitive diagnosis models using log-linear models with latent variables},
  author    = {Henson, Robert A and Templin, Jonathan and Willse, John T},
  journal   = {Psychometrika},
  volume    = {74},
  number    = {2},
  pages     = {191--210},
  year      = {2009},
  publisher = {Springer},
  doi = {10.1007/s11336-008-9089-5}
}
@article{henson2019loglinear,
  title     = {Loglinear cognitive diagnostic model ({LCDM})},
  author    = {Henson, Robert A and Templin, Jonathan},
  journal   = {Handbook of Diagnostic Classification Models: Models and Model Extensions, Applications, Software Packages},
  pages     = {171--185},
  year      = {2019},
  publisher = {Springer}
}
% author={益川 弘如 and 白水 始 and 根本 紘志 and 一柳 智紀 and 北澤 武 and 河\UTF{FA11} 美保},

@article{horn1965parallel,
  title     = {A rationale and test for the number of factors in factor analysis},
  author    = {Horn, John L},
  journal   = {Psychometrika},
  volume    = {30},
  pages     = {179--185},
  year      = {1965},
  publisher = {Springer},
  doi       = {10.1007/BF02289447}
}
@article{Hsu2023-iy,
  title     = {Using a {Bayesian} estimation to examine attribute hierarchies of the 2007 {TIMSS} mathematics test: A demonstration using {R}
               packages},
  author    = {Hsu, Chia-Ling and Chen, Yi-Hsin and Wu, Yi-Jhen},
  journal   = {Practical Assessment, Research, and Evaluation},
  publisher = {scholarworks.umass.edu},
  volume    = 28,
  number    = 1,
  pages     = 11,
  year      = 2023, 
  doi = {10.7275/pare.1265}
}

@book{imai2009english,
  title     = {今井の英文法教室 上},
  author    = {今井 宏},
  year      = {2009},
  publisher = {東進ブックス},
  　address  = {東京},
  　volume   = {1}
}

@article{Ismail2022-vb,
  title     = {Formative vs. summative assessment: impacts on academic
               motivation, attitude toward learning, test anxiety, and
               self-regulation skill},
  author    = {Ismail, Seyed M and Rahul, D R and Patra, Indrajit and Rezvani,
               Ehsan},
  journal   = {Language testing in Asia},
  publisher = {Springer Science and Business Media LLC},
  volume    = 12,
  number    = 1,
  abstract  = {AbstractAs assessment plays an important role in the process of
               teaching and learning, this research explored the impacts of
               formative and summative assessments on academic motivation,
               attitude toward learning, test anxiety, and self-regulation skill
               of EFL students in Iran. To fulfill the objectives of this
               research, 72 Iranian EFL learners were chosen based on the
               convenience sampling method assigned to two experimental groups
               (summative group and formative group) and a control group. Then,
               the groups took the pre-tests of test anxiety, motivation, and
               self-regulation skill. Then, one experimental group was trained
               by following the rules of the formative assessment and the other
               experimental group was taught according to the summative
               assessment. The control group was instructed without using any
               preplanned assessment. After a 15-session treatment, the
               post-tests of the test anxiety, motivation, and self-regulation
               skill were administered to all groups to assess the impacts of
               the instruction on their language achievement. Lastly, a
               questionnaire of attitude was administered to both experimental
               groups to examine their attitudes towards the impacts of
               formative and summative assessment on their English learning
               improvement. The outcomes of one-way ANOVA and Bonferroni tests
               revealed that both summative and formative assessments were
               effective but the formative one was more effective on academic
               motivation, test anxiety, and self-regulation skill. The findings
               of one sample t-test indicated that the participants had positive
               attitudes towards summative and formative assessments. Based on
               the results, it can be concluded that formative assessment is an
               essential part of teaching that should be used in EFL
               instructional contexts. The implications of this study can help
               students to detect their own weaknesses and target areas that
               need more effort and work.},
  month     = sep,
  year      = 2022,
  language  = {en},
  doi = {10.1186/s40468-022-00191-4}
}
@article{junker2001dina,
  title     = {Cognitive assessment models with few assumptions, and connections with nonparametric item response theory},
  author    = {Junker, Brian W and Sijtsma, Klaas},
  journal   = {Applied Psychological Measurement},
  volume    = {25},
  number    = {3},
  pages     = {258--272},
  year      = {2001},
  publisher = {Sage Publications Sage CA: Thousand Oaks, CA},
  doi       = {10.1177/01466210122032064}
}
@book{Kline2016SEM,
  author    = {Rex B Kline},
  title     = {Principles and practice of structural equation modeling},
  publisher = {Guilford Press},
  edition   = {4th ed.},
  year      = 2016
}
@article{kohn2016duality,
  title     = {A proof of the duality of the {DINA} model and the {DINO} model},
  author    = {K{\"o}hn, Hans-Friedrich and Chiu, Chia-Yi},
  journal   = {Journal of Classification},
  volume    = {33},
  number    = 2,
  pages     = {171--184},
  year      = {2016},
  publisher = {Springer},
  doi = {10.1007/s00357-016-9202-x}
}
@article{kohn2017procedure,
  title     = {A procedure for assessing the completeness of the {Q}-matrices of cognitively diagnostic tests},
  author    = {K{\"o}hn, Hans-Friedrich and Chiu, Chia-Yi},
  journal   = {Psychometrika},
  volume    = {82},
  number    = {1},
  pages     = {112--132},
  year      = {2017},
  publisher = {Springer},
  doi       = {10.1007/s11336-016-9536-7}
}
@article{Krosnick1991-fh,
  title     = {Response strategies for coping with the cognitive demands of
               attitude measures in surveys},
  author    = {Krosnick, Jon A},
  journal   = {Applied Cognitive Psychology},
  publisher = {Wiley},
  volume    = 5,
  number    = 3,
  pages     = {213--236},
  abstract  = {This paper proposes that when optimally answering a survey
               question would require substantial cognitive effort, some
               repondents simply provide a satisfactory answer instead. This
               behaviour, called satisficing, can take the form of either (1)
               incomplete or biased information retrieval and/or information
               integration, or (2) no information retrieval or integration at
               all. Satisficing may lead respondents to employ a variety of
               response strategies, including choosing the first response
               alternative that seems to constitute a reasonable answer,
               agreeing with an assertion made by a question, endorsing the
               status quo instead of endorsing social change, failing to
               differentiate among a set of diverse objects in ratings, saying
               ‘don't know’ instead of reporting an opinion, and randomly
               choosing among the response alternatives offered. This paper
               specifies a wide range of factors that are likely to encourage
               satisficing, and reviews relevant evidence evaluating these
               speculations. Many useful directions for future research are
               suggested.},
  month     = may,
  year      = 1991,
  language  = {en},
  doi       = {10.1002/acp.2350050305}
}
@article{kuo2016cognitive,
  title     = {Cognitive diagnostic models for tests with multiple-choice and constructed-response items},
  author    = {Kuo, Bor-Chen and Chen, Chun-Hua and Yang, Chih-Wei and Mok, Magdalena Mo Ching},
  journal   = {Educational Psychology},
  volume    = {36},
  number    = {6},
  pages     = {1115--1133},
  year      = {2016},
  publisher = {Taylor \& Francis}
}

@article{Kuo2018-zj,
  title     = {A Cognitive Diagnosis Model for Identifying Coexisting Skills
               and Misconceptions},
  author    = {Kuo, Bor-Chen and Chen, Chun-Hua and de la Torre, Jimmy},
  abstract  = {At present, most existing cognitive diagnosis models (CDMs) are
               designed to either identify the presence and absence of skills
               or misconceptions, but not both. This article proposes a CDM
               that can be used to simultaneously identify what skills and
               misconceptions students possess. In addition, it proposes the
               use of the expectation-maximization algorithm to estimate the
               model parameters. A simulation study is conducted to evaluate
               the viability of the proposed model and algorithm. Real data are
               analyzed to demonstrate the applicability of the proposed model,
               and compare it with existing CDMs. Furthermore, a real
               data-based simulation study is conducted to determine how the
               correct classification rates in the context of the proposed
               model can be improved. Issues related to the proposed model and
               future research are discussed.},
  journal   = {Applied Psychological Measurement},
  publisher = {journals.sagepub.com},
  volume    = 42,
  number    = 3,
  pages     = {179--191},
  month     = may,
  year      = 2018,
  keywords  = {Bug-DINO; DINA; agreement rate; cognitive diagnosis model;
               expectation-maximization},
  language  = {en},
  doi = {10.1177/0146621617722791}
}

@article{LeeSawaki2009,
  author    = { Yong-Won   Lee  and  Yasuyo   Sawaki },
  title     = {Cognitive Diagnosis Approaches to Language Assessment: An Overview},
  journal   = {Language Assessment Quarterly},
  volume    = {6},
  number    = {3},
  pages     = {172-189},
  year      = {2009},
  publisher = {Routledge},
  doi       = {10.1080/15434300902985108},
  url       = {https://doi.org/10.1080/15434300902985108},
  eprint    = {https://doi.org/10.1080/15434300902985108}
}

@book{leighton2007cognitive,
  title     = {Cognitive diagnostic assessment for education: Theory and applications},
  author    = {Leighton, Jacqueline and Gierl, Mark},
  year      = {2007},
  publisher = {Cambridge University Press},
  address   = {Cambridge},
  doi       = {10.1017/CBO9780511611186}
}


@article{lesage2013scoring,
  title    = {Scoring methods for multiple choice assessment in higher education – Is it still a matter of number right scoring or negative marking?},
  journal  = {Studies in Educational Evaluation},
  volume   = {39},
  number   = {3},
  pages    = {188-193},
  year     = {2013},
  issn     = {0191-491X},
  doi      = {https://doi.org/10.1016/j.stueduc.2013.07.001},
  url      = {https://www.sciencedirect.com/science/article/pii/S0191491X13000254},
  author   = {Ellen Lesage and Martin Valcke and Elien Sabbe},
  keywords = {Assessment, Higher education, Multiple choice},
  abstract = {In higher education, a multiple choice test is a widely known format for measuring student's knowledge. The debate about the two most commonly used scoring methods for multiple choice assessment – number right scoring (NR) and negative marking (NM) – seems to be a never-ending story. Both NR scoring as NM do not seem to meet the expectations. However, available research hardly offers alternative methods. Clearly, there is a growing need to explore these alternative scoring methods in order to inform and support test designers. This review aims to present an overview of (alternative) scoring methods for multiple choice tests, in which strengths and weaknesses of each method are provided.}
}

@article{li2016selection,
  title     = {The selection of cognitive diagnostic models for a reading comprehension test},
  author    = {Li, Hongli and Hunter, C Vincent and Lei, Pui-Wa},
  journal   = {Language Testing},
  volume    = {33},
  number    = {3},
  pages     = {391--409},
  year      = {2016},
  publisher = {Sage Publications Sage UK: London, England},
  doi       = {10.1177/0265532215590848}
}
@article{lindquist2015some,
  title     = {Some notes on corrections for guessing and related problems},
  author    = {Lindquist, EF and Hoover, HD},
  journal   = {Educational Measurement: Issues and Practice},
  volume    = {34},
  number    = {2},
  pages     = {15--19},
  year      = {2015},
  publisher = {Wiley Online Library},
  doi       = {10.1111/emip.12072}
}

@article{liu2021nested,
  title     = {Nested diagnostic classification models for multiple-choice items},
  author    = {Liu, Ren and Liu, Haiyan},
  journal   = {British Journal of Mathematical and Statistical Psychology},
  volume    = {74},
  number    = {2},
  pages     = {257--285},
  year      = {2021},
  publisher = {Wiley Online Library},
  doi       = {10.1111/bmsp.12214}
}
@article{Liu2022-sj,
  title    = {Diagnostic Classification Models for a Mixture of Ordered and
              Non-ordered Response Options in Rating Scales},
  author   = {Liu, Ren and Liu, Haiyan and Shi, Dexin and Jiang, Zhehan},
  journal  = {Applied Psychological Measurement},
  volume   = 46,
  number   = 7,
  pages    = {622--639},
  abstract = {When developing ordinal rating scales, we may include potentially
              unordered response options such as ``Neither Agree nor Disagree,''
              ``Neutral,'' ``Don't Know,'' ``No Opinion,'' or ``Hard to Say.''
              To handle responses to a mixture of ordered and unordered options,
              Huggins-Manley et al. (2018) proposed a class of semi-ordered
              models under the unidimensional item response theory framework.
              This study extends the concept of semi-ordered models into the
              area of diagnostic classification models. Specifically, we propose
              a flexible framework of semi-ordered DCMs that accommodates most
              earlier DCMs and allows for analyzing the relationship between
              those potentially unordered responses and the measured traits.
              Results from an operational study and two simulation studies show
              that the proposed framework can incorporate both ordered and
              non-ordered responses into the estimation of the latent traits and
              thus provide useful information about both the items and the
              respondents.},
  month    = oct,
  year     = 2022,
  keywords = {diagnostic classification model; neutral responses; nominal
              response option; ordinal response option; rating scales;
              semi-ordered model},
  language = {en}
}

@article{Liu2024-rm,
  title     = {Restricted latent class models for nominal response data:
               Identifiability and estimation},
  author    = {Liu, Ying and Culpepper, Steven Andrew},
  journal   = {Psychometrika},
  publisher = {Springer Science and Business Media LLC},
  volume    = 89,
  number    = 2,
  pages     = {592--625},
  abstract  = {Restricted latent class models (RLCMs) provide an important
               framework for diagnosing and classifying respondents on a
               collection of multivariate binary responses. Recent research made
               significant advances in theory for establishing identifiability
               conditions for RLCMs with binary and polytomous response data.
               Multiclass data, which are unordered nominal response data, are
               also widely collected in the social sciences and psychometrics
               via forced-choice inventories and multiple choice tests. We
               establish new identifiability conditions for parameters of RLCMs
               for multiclass data and discuss the implications for substantive
               applications. The new identifiability conditions are applicable
               to a wealth of RLCMs for polytomous and nominal response data. We
               propose a Bayesian framework for inferring model parameters,
               assess parameter recovery in a Monte Carlo simulation study, and
               present an application of the model to a real dataset.},
  month     = jun,
  year      = 2024,
  keywords  = {Bayesian; cognitive diagnosis model; identifiability; nominal
               response data; restricted latent class models},
  language  = {en},
  doi = {10.1007/s11336-023-09940-7}
}

@book{lord1968statistical,
  title     = {Statistical theories of mental test scores},
  author    = {Lord, FM and Novick, MR and Birnbaum, Allan},
  year      = {1968},
  publisher = {Addison-Wesley},
  address      = {Reading, MA}
}

@article{ma2024dual,
  title     = {A Dual-Purpose Model for Binary Data: Estimating Ability and Misconceptions},
  author    = {Ma, Wenchao and Sorrel, Miguel A and Zhai, Xiaoming and Ge, Yuan},
  journal   = {Journal of Educational Measurement},
  year      = {2024},
  publisher = {Wiley Online Library}
}

@article{madison2018assessing,
  title     = {Assessing growth in a diagnostic classification model framework},
  author    = {Madison, Matthew J and Bradshaw, Laine P},
  journal   = {Psychometrika},
  volume    = {83},
  number    = {4},
  pages     = {963--990},
  year      = {2018},
  publisher = {Springer},
  doi       = {10.1007/s11336-018-9638-5}
}

@article{maniaci2014caring,
  title     = {Caring about carelessness: Participant inattention and its effects on research},
  author    = {Maniaci, Michael R and Rogge, Ronald D},
  journal   = {Journal of Research in Personality},
  volume    = {48},
  pages     = {61--83},
  year      = {2014},
  publisher = {Elsevier},
  doi = {10.1016/j.jrp.2013.09.008}
}


@article{maris1999estimating,
  title     = {Estimating multiple classification latent class models},
  author    = {Maris, Eric},
  journal   = {Psychometrika},
  volume    = {64},
  number    = {2},
  pages     = {187--212},
  year      = {1999},
  publisher = {Springer},
  doi = {10.1007/BF02294535},
}
@article{ozaki2015dina,
  title     = {{DINA} models for multiple-choice items with few parameters: Considering incorrect answers},
  author    = {Ozaki, Koken},
  journal   = {Applied Psychological Measurement},
  volume    = {39},
  number    = {6},
  pages     = {431--447},
  year      = {2015},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
  doi       = {10.1177/0146621615574693}
}
 @article{ozaki2020bugmc,
  title     = {Cognitive diagnosis models for estimation of misconceptions analyzing multiple-choice data},
  author    = {Ozaki, Koken and Sugawara, Shingo and Arai, Noriko},
  journal   = {Behaviormetrika},
  volume    = {47},
  number    = {1},
  pages     = {19--41},
  year      = {2020},
  publisher = {Springer},
  doi = {10.1007/s41237-019-00100-9}
}


@article{Ozan2018-km,
  title     = {The effects of formative assessment on academic achievement,
               attitudes toward the lesson, and self-regulation skills},
  author    = {Ozan, Ceyhun and Kıncal, Remzi Y},
  journal   = {Adv. Health Sci. Educ. Theory Pract.},
  publisher = {Egitim Danismanligi ve Arastirmalari (EDAM)},
  volume    = 18,
  number    = 1,
  month     = apr,
  year      = 2018
}

%　MC
@article{park2018explanatory,
  title     = {Explanatory cognitive diagnostic models: Incorporating latent and observed predictors},
  author    = {Park, Yoon Soo and Xing, Kuan and Lee, Young-Sun},
  journal   = {Applied Psychological Measurement},
  volume    = {42},
  number    = {5},
  pages     = {376--392},
  year      = {2018},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA}
}
%　MC
@inproceedings{plummer2003jags,
  title        = {{J}{A}{G}{S}: A program for analysis of {B}ayesian graphical models using {G}ibbs sampling},
  author       = {Plummer, Martyn and others},
  booktitle    = {Proceedings of the 3rd international workshop on distributed statistical computing},
  volume       = {124},
  pages        = {1--8},
  year         = {2003},
  note         = {Retrieved from \url{https://www.r-project.org/conferences/DSC-2003/Drafts/}},
  organization = {Vienna, Austria.}
}

@article{polson2013bayesian,
  title     = {{Bayesian} inference for logistic models using {P}{\'o}lya--{Gamma} latent variables},
  author    = {Polson, Nicholas G and Scott, James G and Windle, Jesse},
  journal   = {Journal of the American statistical Association},
  volume    = {108},
  number    = {504},
  pages     = {1339--1349},
  year      = {2013},
  publisher = {Taylor \& Francis},
  doi = {10.1080/01621459.2013.829001}
}

@manual{R,
  title        = {R: A Language and Environment for Statistical Computing},
  author       = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address      = {Vienna, Austria},
  year         = {2022},
  url          = {https://www.R-project.org/}
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@article{Ravand2020recent,
  title     = {Diagnostic classification models: Recent developments, practical
               issues, and prospects},
  author    = {Ravand, Hamdollah and Baghaei, Purya},
  journal   = {International Journal of Testing},
  publisher = {Informa UK Limited},
  volume    = 20,
  number    = 1,
  pages     = {24--56},
  abstract  = {More than three decades after their introduction, diagnostic
               classification models (DCM) do not seem to have been implemented
               in educational systems for the purposes they were devised. Most
               DCM research is either methodological for model development and
               refinement or retrofitting to existing nondiagnostic tests and,
               in the latter case, basically for model demonstration or
               constructs identification. DCMs have rarely been used to develop
               diagnostic assessment right from the start with the purpose of
               identifying individuals’ strengths and weaknesses (referred to as
               true applications in this study). In this article, we give an
               introduction to DCMs and their latest developments along with
               guidelines on how to proceed to employ DCMs to develop a
               diagnostic test or retrofit to a nondiagnostic assessment.
               Finally, we enumerate the reasons why we believe DCMs have not
               become fully operational in educational systems and suggest some
               advice to make their advent smooth and quick.},
  month     = jan,
  year      = 2020,
  language  = {en},
  doi = {10.1080/15305058.2019.1588278}
}


@incollection{roussos2007fusion,
  title     = {The fusion model skills diagnosis system},
  author    = {Roussos, Louis A and DiBello, Louis V and Stout, William and Hartz, Sarah M and Henson, Robert A and Templin, Jonathan L},
  booktitle = {Cognitive diagnostic assessment for education: Theory and applications},
  address   = {Cambridge},
  editor    = {Leighton, J and Gierl, M},
  pages     = {275--318},
  year      = {2007},
  publisher = {Cambridge University Press}
}

@article{roussos2007skills,
  title     = {Skills diagnosis using {IRT}-based latent class models},
  author    = {Roussos, Louis A and Templin, Jonathan L and Henson, Robert A},
  journal   = {Journal of Educational Measurement},
  volume    = {44},
  number    = {4},
  pages     = {293--311},
  year      = {2007},
  publisher = {Wiley Online Library},
  doi ={10.1111/j.1745-3984.2007.00040.x}
}


@article{Rupp2008Q-matrixMisspecification,
  author   = {André A. Rupp and Jonathan Templin},
  title    = {The Effects of {Q}-Matrix Misspecification on Parameter Estimates and Classification Accuracy in the {DINA} Model},
  journal  = {Educational and Psychological Measurement},
  volume   = {68},
  number   = {1},
  pages    = {78-96},
  year     = {2008},
  doi      = {10.1177/0013164407301545},
  url      = { 
              https://doi.org/10.1177/0013164407301545
              
              },
  eprint   = { 
              https://doi.org/10.1177/0013164407301545
              
              },
  abstract = { This article reports a study that investigated the effects of {Q}-matrix misspecifications on parameter estimates and misclassification rates for the deterministic-input, noisy ``and'' gate ({DINA}) model, which is a restricted latent class model for multiple classifications of respondents that can be useful for cognitively motivated diagnostic assessment. In this study, a {Q}-matrix for an assessment mapping all 15 possible attribute patterns based on four independent attributes was misspecified by changing one ``0'' or ``1'' for each item. This was done in a way that ensured that certain attribute combinations were completely deleted from the {Q}-matrix, and certain incorrect dependency relationships between attributes were represented. Results showed clear effects that included an itemspecific overestimation of slipping parameters when attributes were deleted from the {Q}-matrix, an item-specific overestimation of guessing parameters when attributes were added to the {Q}-matrix, and high misclassification rates for attribute classes that contained attribute combinations that were deleted from the {Q}-matrix. }
}

@article{rupp2008unique,
  title     = {Unique characteristics of diagnostic classification models: A comprehensive review of the current state-of-the-art},
  author    = {Rupp, Andr{\'e} A and Templin, Jonathan},
  journal   = {Measurement},
  volume    = {6},
  number    = {4},
  pages     = {219--262},
  year      = {2008},
  publisher = {Taylor \& Francis},
  doi       = {10.1080/15366360802490866}
}

@book{rupp2010diagnostic,
  title     = {Diagnostic measurement: Theory, methods, and applications},
  author    = {Rupp, Andre A and Templin, Jonathan and Henson, Robert A},
  year      = {2010},
  publisher = {Guilford Press},
  address   = {New York},
  doi       = {10.1111/j.1745-3984.2011.00141.x}
}


@article{Schildkamp2020-ti,
  title    = {Formative assessment: A systematic review of critical teacher
              prerequisites for classroom practice},
  author   = {Schildkamp, Kim and van der Kleij, Fabienne M and Heitink, Maaike
              C and Kippers, Wilma B and Veldkamp, Bernard P},
  abstract = {Formative assessment has the potential to support teaching and
              learning in the classroom. This study reviewed the literature on
              formative assessment to identify prerequisites for effective use
              of formative assessment by teachers. The review sought to address
              the following research question: What teacher prerequisites need
              to be in place for using formative assessment in their classroom
              practice? The review was conducted using a systematic approach. A
              total of 54 studies were included in this review. The results
              show that (1) knowledge and skills (e.g., data literacy), (2),
              psychological factors (e.g., social pressure), and (3) social
              factors (e.g., collaboration) influence the use of formative
              assessment. The prerequisites identified can inform professional
              development initiatives regarding formative assessment, as well
              as teacher education programs.},
  journal  = {International Journal of Educational Research},
  volume   = 103,
  pages    = {101602},
  month    = jan,
  year     = 2020,
  keywords = {Formative assessment; Data-based decision making; Assessment for
              learning; Teacher prerequisites; Classroom practice; Systematic
              review},
  doi = {10.1016/j.ijer.2020.101602}
}


@article{Sen2017-kn,
  title     = {Comparison of relative fit indices for diagnostic model selection},
  author    = {Sen, Sedat and Bradshaw, Laine},
  journal   = {Applied Psychological Measurement},
  publisher = {SAGE Publications},
  volume    = 41,
  number    = 6,
  pages     = {422--438},
  abstract  = {The purpose of this study was to thoroughly examine the
               performance of three information-based fit indices-Akaike's
               Information Criterion (AIC), Bayesian Information Criterion
               (BIC), and sample-size-adjusted BIC (SABIC)-using the log-linear
               cognitive diagnosis model and a set of well-known item response
               theory (IRT) models. Two simulation studies were conducted to
               examine the extent to which relative fit indices can identify the
               generating model under a variety of data conditions and model
               misspecifications. Generally, indices performed better when item
               quality was stronger. When the IRT was the generating model, all
               three indices correctly selected the IRT model for all
               replications. When the true model was a diagnostic classification
               model, for all three fit indices, the multidimensional IRT model
               was incorrectly selected as frequently as 70\% of the
               replications. The results of this study identify situations for
               researchers where commonly used-and typically well-performing-fit
               indices may not be appropriate to compare models for selection.},
  month     = sep,
  year      = 2017,
  keywords  = {diagnostic classification models; log-linear cognitive diagnosis
               model; model selection; relative fit},
  language  = {en},
  doi = {10.1177/0146621617695521}
}


@article{Sessoms2018applicationreview,
  title     = {Applications of Diagnostic Classification Models: A Literature
               Review and Critical Commentary},
  author    = {Sessoms, John and Henson, Robert A},
  journal   = {Measurement: Interdisciplinary Research and Perspectives},
  publisher = {Taylor \& Francis},
  volume    = 16,
  number    = 1,
  pages     = {1--17},
  month     = jan,
  year      = 2018,
  doi = {10.1080/15366367.2018.1435104},
}

@article{shang2021partial,
  title     = {Partial-mastery cognitive diagnosis models},
  author    = {Shang, Zhuoran and Erosheva, Elena A and Xu, Gongjun},
  journal   = {Annals of Applied Statistics},
  volume    = {15},
  number    = {3},
  pages     = {1529--1555},
  year      = {2021},
  publisher = {Institute of Mathematical Statistics},
  doi       = {10.1214/21-AOAS1439}
}

@incollection{Shear2017-mz,
  title     = {Validating a {Distractor-Driven} Geometry Test Using a
               Generalized Diagnostic Classification Model},
  booktitle = {Understanding and Investigating Response Processes in Validation
               Research},
  author    = {Shear, Benjamin R and Roussos, Louis A},
  editor    = {Zumbo, Bruno D and Hubley, Anita M},
  abstract  = {This chapter uses a generalized diagnostic classification model
               (GDCM) to provide validity evidence for a test measuring student
               misconceptions in middle school geometry. The test is an example
               of a ``distractor-driven'' test that includes selected-response
               questions with systematically written incorrect response
               options; scoring the test involves tracking which specific
               response options students select for each item. The test is
               intended to provide teachers with an efficient means of
               obtaining instructionally useful information about their
               students' reasoning, including whether students may be reasoning
               with common misconceptions that could interfere with their
               learning. The GDCM framework provides a way to formally evaluate
               whether student response patterns on the test correspond to the
               proposed test score interpretations. The analyses illustrate how
               graphical and numerical GDCM results can be used to validate
               intended score uses and guide future test development. The
               discussion considers both the strengths and limitations of
               applying the GDCM framework to this type of distractor-driven
               test.},
  publisher = {Springer International Publishing},
  pages     = {277--304},
  year      = 2017,
  address   = {Cham}
}

@article{Shi2021GDINA_R,
  title     = {Cognitively diagnostic analysis using the {G}-{DINA} model in {R}},
  author    = {Shi, Qingzhou and Ma, Wenchao and Robitzsch, Alexander and
               Sorrel, Miguel A and Man, Kaiwen},
  journal   = {Psych},
  publisher = {MDPI AG},
  volume    = 3,
  number    = 4,
  pages     = {812--835},
  abstract  = {Cognitive diagnosis models (CDMs) have increasingly been applied
               in education and other fields. This article provides an overview
               of a widely used CDM, namely, the G-DINA model, and demonstrates
               a hands-on example of using multiple R packages for a series of
               CDM analyses. This overview involves a step-by-step illustration
               and explanation of performing Q-matrix evaluation, CDM
               calibration, model fit evaluation, item diagnosticity
               investigation, classification reliability examination, and the
               result presentation and visualization. Some limitations of
               conducting CDM analysis in R are also discussed.},
  month     = dec,
  year      = 2021,
  language  = {en},
  doi = {10.3390/psych3040052},
}
@incollection{Sinharay2019-sj,
  title     = {Measures of agreement: Reliability, classification accuracy, and
               classification consistency},
  author    = {Sinharay, Sandip and Johnson, Matthew S},
  booktitle = {Handbook of Diagnostic Classification Models: Models and Model Extensions, Applications, Software Packages},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {359--377},
  year      = 2019
}
@article{stevens1946theory,
  title     = {On the theory of scales of measurement},
  author    = {Stevens, Stanley Smith},
  journal   = {Science},
  volume    = {103},
  number    = {2684},
  pages     = {677--680},
  year      = {1946},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.103.2684.677},
}
@article{Stout2023-fe,
  title     = {Three psychometric-model-based option-scored multiple choice item
               design principles that enhance instruction by improving quiz
               diagnostic classification of knowledge attributes},
  author    = {Stout, William and Henson, Robert A and DiBello, Lou},
  journal   = {Psychometrika},
  publisher = {Springer Science and Business Media LLC},
  volume    = 88,
  number    = 4,
  pages     = {1299--1333},
  abstract  = {Three IRT diagnostic-classification-modeling (DCM)-based multiple
               choice (MC) item design principles are stated that improve
               classroom quiz student diagnostic classification. Using
               proven-optimal maximum likelihood-based student classification,
               example items demonstrate that adherence to these item design
               principles increases attribute (skills and especially
               misconceptions) correct classification rates (CCRs). Simple
               formulas compute these needed item CCRs. By use of these
               psychometrically driven item design principles, hopefully enough
               attributes can be accurately diagnosed by necessarily short
               MC-item-based quizzes to be widely instructionally useful. These
               results should then stimulate increased use of well-designed MC
               item quizzes that target accurately diagnosing
               skills/misconceptions, thereby enhancing classroom learning.},
  month     = dec,
  year      = 2023,
  keywords  = {Extended RUM (ERUM); Generalized Diagnostic Classifcation
               Modeling (GDCM); Optimal multiple choice (MC) item (question)
               design; formative assessment; optimal student diagnostic
               classification; skills and misconceptions diagnosis},
  language  = {en},
  doi = {10.1007/s11336-022-09885-3},
}
@article{Stout2023-qz,
  title     = {Optimal classification methods for diagnosing latent skills and
               misconceptions for option-scored multiple-choice item quizzes},
  author    = {Stout, William and Henson, Robert A and DiBello, Lou},
  abstract  = {… As stated above, the general psychometric modeling paradigm of
               this paper is Diagnostic Classification Modeling (DCM): for
               background, see for example the recent DCM-focused …},
  journal   = {Behaviormetrika},
  publisher = {Springer},
  volume    = 50,
  number    = 1,
  pages     = {177--215},
  year      = 2023
}
@article{suzuki2015,
  title   = {連続型の特性値をもつ補償型認知診断モデル},
  author  = {鈴木 雅之 and 豊田 哲也 and 山口 一大 and 孫 媛},
  journal = {日本テスト学会誌},
  volume  = {11},
  number  = {1},
  pages   = {82-96},
  year    = {2015}
}
@article{tan2020,
  author  = {丹 亮人 and 岡田 謙介},
  title   = {連続型の特性値をもつ補償型認知診断モデル},
  journal = {日本テスト学会誌},
  volume  = {16},
  number  = {1},
  pages   = {31-44},
  year    = {2020},
  doi     = {10.24690/jart.16.1_31}
}
@article{tatsuoka1983rule,
  title     = {Rule space: An approach for dealing with misconceptions based on item response theory},
  author    = {Tatsuoka, Kikumi K},
  journal   = {Journal of educational measurement},
  volume    = {20},
  number    = {4},
  pages     = {345--354},
  year      = {1983},
  publisher = {JSTOR},
  doi       = {10.1111/j.1745-3984.1983.tb00212.x}
}
@article{templin2006dino,
  title     = {Measurement of psychological disorders using cognitive diagnosis models},
  author    = {Templin, Jonathan and Henson, Robert A},
  journal   = {Psychological methods},
  volume    = {11},
  number    = {3},
  pages     = {287--305},
  year      = {2006},
  publisher = {American Psychological Association},
  doi       = {10.1037/1082-989X.11.3.287}
}
@inproceedings{templin2008NRDM,
  title     = {Cognitive diagnosis models for nominal response data},
  author    = {Templin, Jonathan and Henson, RA and Rupp, AA and Jang, Eunice and Ahmed, Mushtaq},
  booktitle = {Annual meeting of the National Council on Measurement in Education},
  year      = {2008}
}
@article{templin2013reliability,
  title     = {Measuring the reliability of diagnostic classification model examinee estimates},
  author    = {Templin, Jonathan and Bradshaw, Laine},
  journal   = {Journal of Classification},
  volume    = {30},
  number    = {2},
  pages     = {251--275},
  year      = {2013},
  publisher = {Springer}, 
  doi = {10.1007/s00357-013-9129-4}
}

@article{tsubota2020,
  title   = {多枝選択式問題作成ガイドラインの実証的検討},
  author  = {坪田 彩乃 and 石井 秀宗},
  journal = {日本テスト学会誌},
  volume  = {16},
  number  = {1},
  pages   = {1-12},
  year    = {2020},
  doi     = {10.24690/jart.16.1_1}
}

@article{tsung2017politicalIG,
  issn      = {10471987, 14764989},
  url       = {https://www.jstor.org/stable/26563493},
  author    = {Tsung-han Tsai and Chang-chih Lin},
  journal   = {Political Analysis},
  number    = {4},
  pages     = {483--504},
  publisher = {[Cambridge University Press, Society for Political Methodology]},
  title     = {Modeling Guessing Components in the Measurement of Political Knowledge},
  urldate   = {2023-03-29},
  volume    = {25},
  year      = {2017},
  doi       = {10.1017/pan.2017.21}
}

@article{von-Davier2014-GDM-LCDM,
  title     = {The log-linear cognitive diagnostic model ({LCDM}) as a special
               case of the general diagnostic model ({GDM})},
  author    = {von Davier, Matthias},
  journal   = {ETS Research Report Series},
  publisher = {Wiley},
  volume    = 2014,
  number    = 2,
  pages     = {1--13},
  abstract  = {Diagnostic models combine multiple binary latent variables in an
               attempt to produce a latent structure that provides more
               information about test takers' performance than do unidimensional
               latent variable models. Recent developments in diagnostic
               modeling emphasize the possibility that multiple skills may
               interact in a conjunctive way within the item function, while
               individual skills still may retain separable additive effects.
               This extension of either the conjunctive
               deterministic‐input‐noisy‐and (DINA) model to the generalized
               version (G‐DINA) or the compensatory/additive general diagnostic
               model (GDM) to the log‐linear cognitive diagnostic model (LCDM)
               is aimed at integrating models with conjunctive skills and those
               that assume compensatory functioning of multiple skill variables.
               More recently, a result was proven mathematically that the fully
               conjunctive DINA model, which combines all required skills in a
               single binary function, may be recast as a compensatory special
               case of the GDM. This can be accomplished in more than one form
               such that the resulting transformed skill‐space definitions and
               design (Q) matrices are different from each other but
               mathematically equivalent to the DINA model, producing identical
               model‐based response probabilities. In this report, I extend this
               equivalency result to the LCDM and show that a mathematically
               equivalent, constrained GDM can be defined that yields identical
               parameter estimates based on a transformed set of compensatory
               skills.},
  month     = dec,
  year      = 2014,
  language  = {en},
  doi = {10.1002/ets2.12043},
}

@article{von2008general,
  title     = {A general diagnostic model applied to language testing data},
  author    = {Von Davier, Matthias},
  journal   = {British Journal of Mathematical and Statistical Psychology},
  volume    = {61},
  number    = {2},
  pages     = {287--307},
  year      = {2008},
  publisher = {Wiley Online Library}
}
@book{von2019handbook,
  title     = {Handbook of diagnostic classification models: Models and Model Extensions, Applications, Software Packages},
  author    = {von Davier, Matthias and Lee, Young-Sun},
  journal   = {Springer International Publishing},
  address   = {Cham},
  year      = {2019},
  publisher = {Springer},
  doi       = {10.1007/978-3-030-05584-4}
}
@article{Wang2015-id,
  title     = {Assessing item-level fit for the {DINA} model},
  author    = {Wang, Chun and Shu, Zhan and Shang, Zhuoran and Xu, Gongjun},
  journal   = {Applied Psychological Measurement},
  publisher = {SAGE Publications},
  volume    = 39,
  number    = 7,
  pages     = {525--538},
  abstract  = {This research focuses on developing item-level fit checking
               procedures in the context of diagnostic classification models
               (DCMs), and more specifically for the ``Deterministic Input;
               Noisy 'And' gate'' (DINA) model. Although there is a growing body
               of literature discussing model fit checking methods for DCM, the
               item-level fit analysis is not adequately discussed in
               literature. This study intends to take an initiative to fill in
               this gap. Two approaches are proposed, one stems from classical
               goodness-of-fit test statistics coupled with the
               Expectation-Maximization algorithm for model estimation, and the
               other is the posterior predictive model checking (PPMC) method
               coupled with the Markov chain Monte Carlo estimation. For both
               approaches, the chi-square statistic and a power-divergence index
               are considered, along with Stone's method for considering
               uncertainty in latent attribute estimation. A simulation study
               with varying manipulated factors is carried out. Results show
               that both approaches are promising if Stone's method is imposed,
               but the classical goodness-of-fit approach has a much higher
               detection rate (i.e., proportion of misfit items that are
               correctly detected) than the PPMC method.},
  month     = oct,
  year      = 2015,
  keywords  = {DINA model; chi-square index; correct detection rate; false
               positive rate; posterior predictive model checking;
               power-divergence index},
  language  = {en},
  doi = {10.1177/0146621615583050},
}
@article{Wang2023nonparaMC,
  title     = {Nonparametric Classification Method for Multiple-Choice Items in
               Cognitive Diagnosis},
  author    = {Wang, Yu and Chiu, Chia-Yi and Köhn, Hans Friedrich},
  journal   = {Journal of Educational and Behavioral Statistics},
  publisher = {American Educational Research Association},
  volume    = 48,
  number    = 2,
  pages     = {189--219},
  abstract  = {The multiple-choice (MC) item format has been widely used in
               educational assessments across diverse content domains. MC items
               purportedly allow for collecting richer diagnostic information.
               The effectiveness and economy of administering MC items may have
               further contributed to their popularity not just in educational
               assessment. The MC item format has also been adapted to the
               cognitive diagnosis (CD) framework. Early approaches simply
               dichotomized the responses and analyzed them with a CD model for
               binary responses. Obviously, this strategy cannot exploit the
               additional diagnostic information provided by MC items. De la
               Torre?s MC Deterministic Inputs, Noisy ?And? Gate (MC-DINA) model
               was the first for the explicit analysis of items having MC
               response format. However, as a drawback, the attribute vectors of
               the distractors are restricted to be nested within the key and
               each other. The method presented in this article for the CD of
               DINA items having MC response format does not require such
               constraints. Another contribution of the proposed method concerns
               its implementation using a nonparametric classification
               algorithm, which predestines it for use especially in
               small-sample settings like classrooms, where CD is most needed
               for monitoring instruction and student learning. In contrast,
               default parametric CD estimation routines that rely on EM- or
               MCMC-based algorithms cannot guarantee stable and reliable
               estimates?despite their effectiveness and efficiency when samples
               are large?due to computational feasibility issues caused by
               insufficient sample sizes. Results of simulation studies and a
               real-world application are also reported.},
  month     = apr,
  year      = 2023,
  doi = {10.3102/10769986221133088}
}
@article{wang2023nonparametric,
  title     = {Nonparametric Classification Method for Multiple-Choice Items in Cognitive Diagnosis},
  author    = {Wang, Yu and Chiu, Chia-Yi and K{\"o}hn, Hans Friedrich},
  journal   = {Journal of Educational and Behavioral Statistics},
  volume    = {48},
  number    = {2},
  pages     = {189--219},
  year      = {2023},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA}
}

@article{watanabe2010WAIC,
  title   = {Asymptotic equivalence of {B}ayes cross validation and widely applicable information criterion in singular learning theory.},
  author  = {Watanabe, Sumio and Opper, Manfred},
  journal = {Journal of Machine Learning Research},
  volume  = {11},
  number  = {12},
  year    = {2010}
}

@article{watanabe2013WBIC,
  author     = {Watanabe, Sumio},
  title      = {A Widely Applicable {Bayesian} Information Criterion},
  year       = {2013},
  issue_date = {January 2013},
  publisher  = {JMLR.org},
  volume     = {14},
  number     = {1},
  issn       = {1532-4435},
  journal    = {Journal of Machine Learning Research},
  pages      = {867–897},
  numpages   = {31},
  keywords   = {widely applicable Bayes information criterion, Bayes marginal likelihood}
}
@article{Wiliam2011-ip,
  title     = {What is assessment for learning?},
  author    = {Wiliam, Dylan},
  journal   = {Studies in Educational Evaluation},
  publisher = {Elsevier BV},
  volume    = 37,
  number    = 1,
  pages     = {3--14},
  abstract  = {The idea that assessment is intrinsic to effective instruction is
               traced from early experiments in the individualization of
               learning through the work of Benjamin Bloom to reviews of the
               impact of feedback on learners in classrooms. While many of these
               reviews detailed the adverse impact of assessment on learning,
               they also indicated that under certain conditions assessment had
               considerable potential to enhance learning. It is shown that
               understanding the impact that assessment has on learning requires
               a broader focus than the feedback intervention itself,
               particularly the learner's responses to the feedback, and the
               learning milieu in which the feedback operates. Different
               definitions of the terms “formative assessment” and “assessment
               for learning” are discussed, and subsumed within a broad
               definition that focuses on the extent to which instructional
               decisions are supported by evidence. The paper concludes by
               exploring some of the consequences of this definition for
               classroom practice.},
  month     = mar,
  year      = 2011,
  language  = {en},
  doi = {10.1016/j.stueduc.2011.03.001},
}
@article{wu2019modeling,
  title     = {Modeling Partial Knowledge on Multiple-Choice Items Using Elimination Testing},
  author    = {Wu, Qian and De Laet, Tinne and Janssen, Rianne},
  journal   = {Journal of Educational Measurement},
  volume    = {56},
  number    = {2},
  pages     = {391--414},
  year      = {2019},
  publisher = {Wiley Online Library},
  doi       = {10.1111/jedm.12213}
}
@article{wu2021certainty,
  title     = {Certainty-Based Marking on Multiple-Choice Items: Psychometrics Meets Decision Theory},
  author    = {Wu, Qian and Vanerum, Monique and Agten, Anouk and Christiansen, Andr{\'e}s and Vandenabeele, Frank and Rigo, Jean-Michel and Janssen, Rianne},
  journal   = {Psychometrika},
  volume    = {86},
  number    = {2},
  pages     = {518--543},
  year      = {2021},
  publisher = {Springer},
  doi       = {10.1007/s11336-021-09759-0}
}
@article{yamaguchi2017misspe,
  title   = {認知診断モデルにおける{Q} 行列の誤設定が診断精度に与える影響},
  author  = {山口 一大},
  journal = {日本テスト学会誌},
  volume  = {13},
  number  = {1},
  pages   = {17-32},
  year    = {2017},
  doi     = {10.24690/jart.13.1_17}
}
@article{yamaguchi2020dina,
  title     = {Variational {B}ayes inference for the {DINA} model},
  author    = {Yamaguchi, Kazuhiro and Okada, Kensuke},
  journal   = {Journal of Educational and Behavioral Statistics},
  volume    = {45},
  number    = {5},
  pages     = {569--597},
  year      = {2020},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
  doi       = {10.3102/1076998620911934}
}
@article{yamaguchi2020mc,
  title     = {Variational {B}ayesian inference for the multiple-choice {DINA} model},
  author    = {Yamaguchi, Kazuhiro},
  journal   = {Behaviormetrika},
  volume    = {47},
  number    = {1},
  pages     = {159--187},
  year      = {2020},
  publisher = {Springer},
  doi       = {10.1007/s41237-020-00104-w}
}
@article{yamaguchi2020variational,
  title     = {Variational {B}ayes inference algorithm for the saturated diagnostic classification model},
  author    = {Yamaguchi, Kazuhiro and Okada, Kensuke},
  journal   = {Psychometrika},
  volume    = {85},
  number    = {4},
  pages     = {973--995},
  year      = {2020},
  publisher = {Springer},
  doi       = {10.3102/1076998620911934}
}
@article{Yamaguchi2022monotonicity,
  title     = {A Gibbs sampling algorithm with monotonicity constraints for
               diagnostic classification models},
  author    = {Yamaguchi, Kazuhiro and Templin, Jonathan},
  journal   = {J. Classification},
  publisher = {Springer Science and Business Media LLC},
  volume    = 39,
  number    = 1,
  pages     = {24--54},
  month     = mar,
  year      = 2022,
  language  = {en}
}


@article{yamaguchi2022reliability,
  title     = {Observed score reliability indices in diagnostic classification models},
  author    = {Yamaguchi, Kazuhiro and Templin, Jonathan},
  journal   = {Behaviormetrika},
  volume    = {49},
  number    = {1},
  pages     = {47--68},
  year      = {2022},
  publisher = {Springer},
  doi       = {10.1007/s41237-021-00153-9}
}
@article{yigit2019CDCAT,
  title     = {Computerized adaptive testing for cognitively based multiple-choice data},
  author    = {Yigit, Hulya D and Sorrel, Miguel A and de la Torre, Jimmy},
  journal   = {Applied Psychological Measurement},
  volume    = {43},
  number    = {5},
  pages     = {388--401},
  year      = {2019},
  publisher = {Sage Publications Sage CA: Los Angeles, CA},
  doi       = {10.1177/0146621618798665}
}
@article{zhai2021validating,
  title     = {Validating a partial-credit scoring approach for multiple-choice science items: an application of fundamental ideas in science},
  author    = {Zhai, Xiaoming and Li, Min},
  journal   = {International Journal of Science Education},
  volume    = {43},
  number    = {10},
  pages     = {1640--1666},
  year      = {2021},
  publisher = {Taylor \& Francis},
  doi       = {10.1214/21-AOAS1439}
}
@article{zhan2019using,
  title     = {Using {J}{A}{G}{S} for {B}ayesian cognitive diagnosis modeling: A tutorial},
  author    = {Zhan, Peida and Jiao, Hong and Man, Kaiwen and Wang, Lijun},
  journal   = {Journal of Educational and Behavioral Statistics},
  volume    = {44},
  number    = {4},
  pages     = {473--503},
  year      = {2019},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
  doi       = {10.3102/1076998619826040}
}
@article{zhan2020partial,
  title     = {A partial mastery, higher-order latent structural model for polytomous attributes in cognitive diagnostic assessments},
  author    = {Zhan, Peida and Wang, Wen-Chung and Li, Xiaomin},
  journal   = {Journal of Classification},
  volume    = {37},
  number    = {2},
  pages     = {328--351},
  year      = {2020},
  publisher = {Springer},
  doi       = {10.1007/s00357-019-09323-7}
}
@book{zimowski1998bilog,
  title     = {{BILOG-MG}: Multiple-group IRT analysis and test maintenance for binary items},
  author    = {Zimowski, Michele F},
  year      = {1998},
  publisher = {Scientific Software International}
}
@article{荒井2020思考力,
  title     = {思考力を測ろうとする多枝選択式問題の解答過程に関する調査に基づく実証分析},
  author    = {荒井清佳},
  journal   = {日本テスト学会誌},
  volume    = {16},
  number    = {1},
  pages     = {14--30},
  year      = {2020},
  publisher = {日本テスト学会}
}
@article{福島健太郎2021q,
  title     = {{Q} 行列を付与した多枝選択形式テストの開発: 認知診断モデルのための英語の空所補充問題の作成},
  author    = {福島健太郎 and 内田奈緒 and 岡田謙介},
  journal   = {日本テスト学会誌},
  volume    = {17},
  number    = {1},
  pages     = {45--59},
  year      = {2021},
  publisher = {日本テスト学会}
}
@article{de2016generalQ,
  title={A general method of empirical {Q}-matrix validation},
  author={de la Torre, Jimmy and Chiu, Chia-Yi},
  journal={Psychometrika},
  volume={81},
  number = {2},
  pages={253--273},
  year={2016},
  publisher={Springer},
  doi = {10.1007/s11336-015-9467-8}
}
@article{wang2020q,
  title={{Q}-matrix estimation methods for cognitive diagnosis models: Based on partial known {Q}-matrix},
  author={Wang, Daxun and Cai, Yan and Tu, Dongbo},
  journal={Multivariate Behavioral Research},
  pages={1--13},
  number = {3},
  year={2020},
  publisher={Taylor \& Francis},
  doi = {10.1080/00273171.2020.1746901}
}

@ARTICLE{Klingbeil2024-na,
  title     = "Validity of multiple-choice digital formative assessment for
               assessing students’ (mis)conceptions: evidence from a
               mixed-methods study in algebra",
  author    = "Klingbeil, Katrin and Rösken, Fabian and Barzel, Bärbel and
               Schacht, Florian and Stacey, Kaye and Steinle, Vicki and Thurm,
               Daniel",
  journal   = "ZDM -- Mathematics Education",
  publisher = "Springer Science and Business Media LLC",
  volume    =  56,
  number    =  4,
  doi = {10.1007/s11858-024-01556-0},
  pages     = "713--726",
  abstract  = "AbstractAssessing students’ (mis)conceptions is a challenging
               task for teachers as well as for researchers. While individual
               assessment, for example through interviews, can provide deep
               insights into students’ thinking, this is very time-consuming and
               therefore not feasible for whole classes or even larger settings.
               For those settings, automatically evaluated multiple-choice (MC)
               items could be a solution. However, it is a challenge to design
               those items and to adapt them for other countries in a way that
               they adequately reveal students’ (mis)conceptions. In this
               article, we investigate the question whether it is valid to use a
               German adaption of a multiple-choice test developed in Australia
               for formative assessment of the letter-as-object misconception in
               Germany. For this, first semi-structured interviews with five
               German Year 8 students were conducted, and second, 616 students
               were asked for short written explanations. These data were
               analysed with regards to the students’ (mis)conceptions and
               compared with their automatic online diagnosis. In general, a
               high concordance between online SMART test results and students’
               explanations was observed, confirming that useful diagnoses of
               student misconceptions can be obtained from such a short
               well-designed MC test.",
  month     =  aug,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Ren2021-rq,
  title     = "Remedial teaching and learning from a cognitive diagnostic model
               perspective: Taking the data distribution characteristics as an
               example",
  author    = "Ren, He and Xu, Ningning and Lin, Yuxiang and Zhang, Shumei and
               Yang, Tao",
  journal   = "Frontiers in Psychology",
  publisher = "Frontiers Media SA",
  volume    =  12,
  pages     =  628607,
  month     =  mar,
  year      =  2021,
  doi = {10.3389/fpsyg.2021.628607},
  keywords  = "DINA; cognitive diagnostic models; data distribution
               characteristics; formative assessment; mathematics teaching",
  language  = "en"
}

@ARTICLE{Fan2021-yb,
  title     = "Integrating diagnostic assessment into curriculum: a theoretical framework and teaching practices",
  author    = "Fan, Tingting and Song, Jieqing and Guan, Zheshu",
  journal   = "Language Testing in Asia",
  publisher = "Springer Science and Business Media LLC",
  volume    =  11,
  number    =  2,
  month     =  jan,
  year      =  2021,
  language  = "en",
  doi = {10.1186/s40468-020-00117-y},
}

@ARTICLE{Tan2023-al,
  title     = "A tutorial on cognitive diagnosis modeling for characterizing
               mental health symptom profiles using existing item responses",
  author    = "Tan, Zhengqi and de la Torre, Jimmy and Ma, Wenchao and Huh,
               David and Larimer, Mary E and Mun, Eun-Young",
  journal   = "Prevention Science",
  publisher = "Springer Science and Business Media LLC",
  volume    =  24,
  number    =  3,
  doi = {10.1007/s11121-022-01346-8},
  pages     = "480--492",
  abstract  = "In research applications, mental health problems such as
               alcohol-related problems and depression are commonly assessed and
               evaluated using scale scores or latent trait scores derived from
               factor analysis or item response theory models. This tutorial
               paper demonstrates the use of cognitive diagnosis models (CDMs)
               as an alternative approach to characterizing mental health
               problems of young adults when item-level data are available.
               Existing measurement approaches focus on estimating the general
               severity of a given mental health problem at the scale level as a
               unidimensional construct without accounting for other symptoms of
               related mental health problems. The prevailing approaches may
               ignore clinically meaningful presentations of related symptoms at
               the item level. The current study illustrates CDMs using
               item-level data from college students (40 items from 719
               respondents; 34.6\% men, 83.9\% White, and 16.3\% first-year
               students). Specifically, we evaluated the constellation of four
               postulated domains (i.e., alcohol-related problems, anxiety,
               hostility, and depression) as a set of attribute profiles using
               CDMs. After accounting for the impact of each attribute (i.e.,
               postulated domain) on the estimates of attribute profiles, the
               results demonstrated that when items or attributes have limited
               information, CDMs can utilize item-level information in the
               associated attributes to generate potentially meaningful
               estimates and profiles, compared to analyzing each attribute
               independently. We introduce a novel visual inspection aid, the
               lens plot, for quantifying this gain. CDMs may be a useful
               analytical tool to capture respondents' risk and resilience for
               prevention research.",
  month     =  apr,
  year      =  2023,
  keywords  = "Assessment; CDM; Classification; Co-occurring symptom profiles;
               DCM; Diagnostic classification model; IRT; Research domain
               criteria",
  language  = "en"
}


@book{kolen2013test,
  title={Test equating, scaling, and linking: Methods and practices},
  author={Kolen, Michael J and Brennan, Robert L},
  year={2014},
  edition = {3},
  publisher={Springer Science \& Business Media}
}

@ARTICLE{Robitzsch2024,
  title     = "A comparison of mixed and partial membership diagnostic
               classification models with multidimensional item response models",
  author    = "Robitzsch, Alexander",
  journal   = "Information",
  publisher = "MDPI AG",
  volume    =  15,
  number    =  6,
  pages     =  331,
  doi = {10.3390/info15060331},
  abstract  = "Diagnostic classification models (DCM) are latent structure
               models with discrete multivariate latent variables. Recently,
               extensions of DCMs to mixed membership have been proposed. In
               this article, ordinary DCMs, mixed and partial membership models,
               and multidimensional item response theory (IRT) models are
               compared through analytical derivations, three example datasets,
               and a simulation study. It is concluded that partial membership
               DCMs are similar, if not structurally equivalent, to sufficiently
               complex multidimensional IRT models.",
  month     =  jun,
  year      =  2024,
  language  = "en"
}

@article{yamaguchi2018TIMSS,
  author = {Yamaguchi, Kazuhiro and Okada, Kensuke},
  journal = {PLoS ONE},
  number = {2},
  title = {Comparison among cognitive diagnostic models for the TIMSS 2007 fourth grade mathematics assessment},
  volume = {13},
  pages = {e0188691},
  year = {2018},
  doi = {10.1371/journal.pone.0188691},
}


@ARTICLE{Cai2010-tn,
  title     = "Metropolis-Hastings Robbins-Monro algorithm for confirmatory item
               factor analysis",
  author    = "Cai, Li",
  journal   = "Journal of Educational and Behavioral Statistics",
  publisher = "American Educational Research Association (AERA)",
  volume    =  35,
  number    =  3,
  pages     = "307--335",
  month     =  jun,
  year      =  2010,
  doi = {10.3102/1076998609353115},
  language  = "en"
}

@misc{Stacey2013SMART,
  author       = {Stacey, Kaye and Price, Barbara and Gvozdenko, Eugenia and Steinle, Vicki},
  title        = {Specific Mathematics Assessments that Reveal Thinking},
  year         = {2013},
  url          = {http://www.smartvic.com/teacher/},
  note         = {Accessed: 2025/1/23},
}

@ARTICLE{Goretzko2025-me,
  title   = "How many factors to retain in exploratory factor analysis? A
             critical overview of factor retention methods",
  author  = "Goretzko, D",
  journal = "Psychological Methods",
  year    =  {2025},
  note={Advance online publication},
  doi = {10.1037/met0000733},
}

@article{Goretzko2020,
  author    = {Daniel Goretzko and Markus Bühner},
  title     = {One model to rule them all? Using machine learning algorithms to determine the number of factors in exploratory factor analysis},
  journal   = {Psychological Methods},
  volume    = {25},
  number    = {6},
  pages     = {776--786},
  year      = {2020},
  doi       = {10.1037/met0000262}
}

@article{Golino2020,
  author    = {Hudson Golino and Dongfang Shi and Alexander P. Christensen and Luis E. Garrido and Miguel D. Nieto and Ruchi Sadana and Jayaprakash A. Thiyagarajan and Antonio Martinez-Molina},
  title     = {Investigating the performance of exploratory graph analysis and traditional techniques to identify the number of latent factors: A simulation and tutorial},
  journal   = {Psychological Methods},
  volume    = {25},
  number    = {3},
  pages     = {292--320},
  year      = {2020},
  doi       = {10.1037/met0000255}
}

@article{Shang2021,
  author    = {Shang, Zhiyong and Erosheva, Elena A. and Xu, Gongjun},
  title     = {Partial-mastery cognitive diagnosis models},
  journal   = {Annals of Applied Statistics},
  year      = {2021},
  volume    = {15},
  pages     = {1529--1555},
  doi       = {10.1214/21-AOAS1439},
}

@book{imai2009,
  author    = {Imai, H.},
  title     = {Imai no eibunpo kyoushitsu},
  year      = {2009},
  publisher = {Toshin Books}
}